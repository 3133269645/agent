import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import os
import re
import openai
import numpy as np
from typing import List, Dict
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


# --- å·¥å…·å‡½æ•°ï¼šçˆ¬è™«è„šæœ¬å°è£… ---
def run_sztu_news_spider():
    """
    çˆ¬å–æ·±åœ³æŠ€æœ¯å¤§å­¦ (sztu.edu.cn) 'æŠ€å¤§ç„¦ç‚¹' æ¿å—çš„æ–°é—»å†…å®¹ã€‚
    å°†æ–°é—»è¯¦æƒ…ä¿å­˜ä¸ºå•ç‹¬çš„ .txt æ–‡ä»¶ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªæ ‡é¢˜åˆ—è¡¨æ–‡ä»¶ã€‚
    è¯¥å‡½æ•°æ— ä»»ä½•å…¥å‚ï¼Œç›´æ¥è°ƒç”¨å³å¯è§¦å‘æ•´ä¸ªçˆ¬è™«æµç¨‹ã€‚
    """

    # --- 1. å®šä¹‰å¸¸é‡ ---
    BASE_URL = "https://www.sztu.edu.cn/"
    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36 Edg/141.0.0.0'
    }
    OUTPUT_DIR = "./data/text_æŠ€å¤§ç„¦ç‚¹"
    TITLE_LIST_FILE = "text_title_list.txt"  # æ ‡é¢˜åˆ—è¡¨æ–‡ä»¶å

    # --- 2. è¾…åŠ©å‡½æ•°å®šä¹‰ ---

    def fetch_list_page(url, headers):
        """è¯·æ±‚åˆ—è¡¨é¡µ HTML å†…å®¹ã€‚"""
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = 'utf-8'
        return response.text

    def parse_list_page(html_content):
        """ä»åˆ—è¡¨é¡µæå–æ–°é—»æ ‡é¢˜ã€æ‘˜è¦å’Œå®Œæ•´é“¾æ¥ã€‚"""
        if not html_content:
            return []

        soup = BeautifulSoup(html_content, 'html.parser')
        list_items = soup.select('li > a')

        extracted_data = []
        for item_a in list_items:
            relative_href = item_a.get('href')
            full_url = urljoin(BASE_URL, relative_href) if relative_href else None

            h3_tag = item_a.select_one('.yy-ifo h3')
            title = h3_tag.text.strip() if h3_tag else 'N/A'

            p_tag = item_a.select_one('.yy-ifo p')
            summary = p_tag.text.strip() if p_tag else 'N/A'

            if full_url and title != 'N/A':
                extracted_data.append({
                    'full_url': full_url,
                    'title': title,
                    'summary': summary
                })
        return extracted_data

    def fetch_detail_page_and_parse(url, headers):
        """è¯·æ±‚è¯¦æƒ…é¡µï¼Œç²¾ç¡®æå–å¹¶æ¸…æ´—æ–°é—»æ­£æ–‡ã€‚"""
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')

        # ç²¾ç¡®åœ°å®šä½æ–°é—»æ­£æ–‡å†…å®¹åŒºåŸŸçš„çˆ¶çº§å®¹å™¨
        content_container = soup.find(class_='content-pg')
        if not content_container:
            content_container = soup.find('form', attrs={'name': '_newscontent_fromname'})
        if not content_container:
            return None, "N/A"

        # 1. æå–æ–‡ç« å‘å¸ƒæ—¥æœŸ
        date_p = content_container.find('div', class_='c-ifo')
        if date_p:
            date_match = re.search(r'æ—¶é—´:\s*(\d{4}/\d{2}/\d{2})', date_p.get_text())
            date_str = date_match.group(1).replace('/', '-') if date_match else "æœªçŸ¥æ—¥æœŸ"

        # 2. æ¸…æ´—æ­£æ–‡å†…å®¹
        all_paragraphs = content_container.find_all('p')
        cleaned_text_lines = []
        EXCLUDE_CLASSES = ['flex', 'bounce']
        EXCLUDE_TEXTS = ['ä¿¡æ¯æ¥æº:', 'ä¾›ç¨¿', 'ç¼–è¾‘', 'æµè§ˆé‡:', 'å›¾ç‰‡æ¥æº', 'HIGHLIGHTS']

        for p_tag in all_paragraphs:
            p_text = p_tag.get_text(strip=True)
            tag_classes = p_tag.get('class', [])

            # è¿‡æ»¤æ‰è¾…åŠ©ä¿¡æ¯ã€ç©ºè¡ŒåŠç‰¹æ®Šå…³é”®è¯
            if not p_text or any(cls in tag_classes for cls in EXCLUDE_CLASSES) or \
                    any(text_fragment in p_text for text_fragment in EXCLUDE_TEXTS) or \
                    re.match(r'^\d{4}-\d{2}-\d{2}$', p_text):
                continue

            # æ’é™¤ä¿¡æ¯æ ä¸­çš„é‡å¤æ®µè½
            if date_p and date_p.find(text=p_text, recursive=True):
                continue

            cleaned_text_lines.append(p_text)

        full_content = "\n\n".join(cleaned_text_lines)
        return full_content, date_str

    def save_article_file(title, content):
        """å°†æ–‡ç« å†…å®¹ä¿å­˜åˆ°ä»¥ [æ—¥æœŸ]_[æ ‡é¢˜].txt å‘½åçš„æ–‡ä»¶ã€‚"""
        # æ¸…ç†æ ‡é¢˜ä¸­çš„éæ³•å­—ç¬¦
        safe_title = re.sub(r'[\\/:*?"<>|]', '', title).strip()

        # æ„é€ æ–‡ä»¶åå’Œè·¯å¾„
        filename = f"{safe_title}.txt"
        filepath = os.path.join(OUTPUT_DIR, filename)

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™è·³è¿‡ï¼ˆé¿å…é‡å¤çˆ¬å–ï¼‰
        if os.path.exists(filepath):
            print(f"âš ï¸ æ–‡ç« å·²å­˜åœ¨ï¼Œè·³è¿‡çˆ¬å–: {filename}")
            return False  # è¿”å› False è¡¨ç¤ºæœªè¿›è¡Œæ–°çš„ä¿å­˜

        with open(filepath, 'w', encoding='utf-8') as f:
            # åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ æ ‡é¢˜å’Œæ—¥æœŸï¼Œä¿æŒæ¸…æ™°ç»“æ„
            f.write(f"ã€æ ‡é¢˜ã€‘: {title}\n")
            f.write(f"ã€æ—¥æœŸã€‘: {date_str}\n\n")
            f.write(content)
        print(f"ğŸ‰ æ–‡ç« æ–‡ä»¶å·²æˆåŠŸä¿å­˜: {filepath}")
        return True  # è¿”å› True è¡¨ç¤ºè¿›è¡Œäº†æ–°çš„ä¿å­˜


    def update_title_list(new_titles):
        """å°†æ–°æå–åˆ°çš„æ ‡é¢˜è¿½åŠ ä¿å­˜åˆ° text_title_list.txt æ–‡ä»¶ä¸­ã€‚"""
        filepath = os.path.join(OUTPUT_DIR, TITLE_LIST_FILE)

        # ç¡®å®šèµ·å§‹ç¼–å· (ç”¨äºè¿½åŠ æ—¶çš„æ­£ç¡®åºå·)
        current_titles_count = 0
        if os.path.exists(filepath):
            with open(filepath, 'r', encoding='utf-8') as f:
                # ç®€å•è®¡ç®—å½“å‰æ–‡ä»¶å·²æœ‰çš„æ ‡é¢˜æ•°é‡ï¼Œä»¥ä¾¿ä»æ­£ç¡®çš„åºå·å¼€å§‹è¿½åŠ 
                for line in f:
                    if re.match(r'^\d+\.', line.strip()):
                        current_titles_count += 1

        # æ ¸å¿ƒï¼šä½¿ç”¨è¿½åŠ æ¨¡å¼ 'a' æ‰“å¼€æ–‡ä»¶
        with open(filepath, 'a', encoding='utf-8') as f:
            # å¦‚æœæ–‡ä»¶æ˜¯ç©ºçš„æˆ–ä¸å­˜åœ¨ï¼Œåˆ™æ·»åŠ å¤´éƒ¨
            if current_titles_count == 0:
                f.write("--- æ–‡ç« æ ‡é¢˜åˆ—è¡¨ ---\n\n")

            start_index = current_titles_count + 1
            for index, title in enumerate(new_titles):
                f.write(f"{start_index + index}. {title}\n")

        print(f"\nâœ… {len(new_titles)} ä¸ªæ–°æ ‡é¢˜å·²è¿½åŠ åˆ°åˆ—è¡¨æ–‡ä»¶: {filepath}")

    # --- ä¸»æ‰§è¡Œé€»è¾‘ ---

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # ç”¨äºä¿å­˜æœ¬æ¬¡è¿è¡Œä¸­æˆåŠŸæ–°å¢çš„æ–‡ç« æ ‡é¢˜
    newly_processed_titles = []

    # 1. çˆ¬å–åˆ—è¡¨é¡µ (ä¿®æ­£å¾ªç¯é€»è¾‘ï¼Œé¿å…è¦†ç›–)
    print("âœ¨ å¼€å§‹çˆ¬å–æ–°é—»åˆ—è¡¨é¡µï¼Œå¤„ç†æ–°çš„æ–‡ç« ...")

    for i in range(1, 108):
        try:
            TARGET_URL = urljoin(BASE_URL, f"jdjd/xyxw/{i}.htm")
            print(f"--- æ­£åœ¨å¤„ç†åˆ—è¡¨é¡µ: {TARGET_URL} ---")

            list_html = fetch_list_page(TARGET_URL, HEADERS)
            news_list = parse_list_page(list_html)

            if not news_list:
                print("ğŸš« æœªæå–åˆ°ä»»ä½•æ–°é—»æ•°æ®æˆ–å·²è¾¾åˆ—è¡¨æœ«å°¾ã€‚")
                break  # åˆ—è¡¨ä¸ºç©ºï¼Œå¯èƒ½çˆ¬å–å®Œæ¯•ï¼Œé€€å‡ºå¾ªç¯

            # 2. éå†å¹¶å¤„ç†æ‰€æœ‰æ–°é—»
            for item in news_list:
                title = item['title']
                full_url = item['full_url']

                # 3. çˆ¬å–è¯¦æƒ…é¡µå¹¶æå–çº¯æ–‡æœ¬å†…å®¹åŠæ—¥æœŸ
                content, date_str = fetch_detail_page_and_parse(full_url, HEADERS)

                # 4. ä¿å­˜æ–‡ä»¶å¹¶è®°å½•æ ‡é¢˜ (save_article_file å†…éƒ¨ä¼šæ£€æŸ¥é‡å¤)
                if content and content.strip():
                    if save_article_file(title, content):
                        # åªæœ‰æˆåŠŸä¿å­˜çš„æ–°æ–‡ç« æ‰åŠ å…¥åˆ—è¡¨
                        newly_processed_titles.append(title)
                else:
                    print(f"âš ï¸ è·³è¿‡ä¿å­˜ ({title})ï¼šè¯¦æƒ…é¡µå†…å®¹æå–å¤±è´¥æˆ–ä¸ºç©ºã€‚")

        except requests.RequestException as e:
            print(f"âŒ åˆ—è¡¨é¡µ {TARGET_URL} è¯·æ±‚å¤±è´¥ï¼Œè·³è¿‡: {e}")
            continue

    # 5. ç»Ÿä¸€æ›´æ–°æ ‡é¢˜åˆ—è¡¨æ–‡ä»¶ (ä½¿ç”¨è¿½åŠ æ¨¡å¼)
    if newly_processed_titles:
        update_title_list(newly_processed_titles)
        print(f"\nğŸ‰ çˆ¬è™«æµç¨‹ç»“æŸï¼Œå…±æ–°å¢ {len(newly_processed_titles)} ç¯‡æ–‡ç« ã€‚")
    else:
        print("\nğŸ‰ çˆ¬è™«æµç¨‹ç»“æŸï¼Œæœ¬æ¬¡è¿è¡Œæœªå‘ç°æ–°çš„æ–‡ç« éœ€è¦ä¿å­˜ã€‚")


# --- è¯­ä¹‰æœç´¢å·¥å…·å‡½æ•° (ä¿æŒä¸å˜) ---

EMBEDDING_MODEL = "text-embedding-3-small"
api_key = os.getenv("OPENAI_API_KEY")
TITLE_LIST_FILE = "./data/text_æŠ€å¤§ç„¦ç‚¹/text_title_list.txt"
CONTENT_BASE_DIR = os.path.dirname(TITLE_LIST_FILE)

def search_jiaodian_news(
        query_text: str,
        top_k: int = 3
) -> List[Dict]:
    """
    é€šè¿‡è¯­ä¹‰æœç´¢ä»æ ‡é¢˜åˆ—è¡¨ä¸­æ£€ç´¢æœ€ç›¸ä¼¼çš„æ ‡é¢˜ï¼Œå¹¶è¯»å–å¯¹åº”æ–‡ä»¶çš„å…¨æ–‡å†…å®¹ã€‚
    """

    # 1. åˆå§‹åŒ–å®¢æˆ·ç«¯å’Œè¯»å–æ ‡é¢˜åˆ—è¡¨
    client = openai.OpenAI(api_key=api_key)

    try:
        with open(TITLE_LIST_FILE, 'r', encoding='utf-8') as f:
            titles_list = [line.strip() for line in f if line.strip()]
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ ‡é¢˜åˆ—è¡¨æ–‡ä»¶æœªæ‰¾åˆ°: {TITLE_LIST_FILE}")
        return []

    if not titles_list:
        print("è­¦å‘Šï¼šæ ‡é¢˜åˆ—è¡¨ä¸ºç©ºã€‚")
        return []

    # 2. ç”Ÿæˆ Embedding å¹¶è®¡ç®—ç›¸ä¼¼åº¦ (æ£€ç´¢æ­¥éª¤)
    all_texts = titles_list + [query_text]
    try:
        response = client.embeddings.create(
            input=all_texts,
            model=EMBEDDING_MODEL
        )
    except Exception as e:
        print(f"Embedding API è°ƒç”¨å¤±è´¥: {e}")
        return []

    title_embeddings = np.array([item.embedding for item in response.data[:-1]])
    query_vector = np.array(response.data[-1].embedding)
    similarity_scores = np.dot(title_embeddings, query_vector)
    ranked_indices = np.argsort(similarity_scores)[::-1]

    # 3. éå†æ£€ç´¢ç»“æœï¼Œè¯»å–å…¨æ–‡å¹¶ç»„è£…æœ€ç»ˆç»“æœ
    final_results = []

    for i in range(min(top_k, len(titles_list))):
        index = ranked_indices[i]
        title = titles_list[index]
        score = round(float(similarity_scores[index]), 4)

        cleaned_title = re.sub(r"^\d+\.\s*", "", title)
        # --- å…¨æ–‡è¯»å–é€»è¾‘ (å†…è”) ---
        file_name = f"{cleaned_title.strip()}.txt"
        full_path = os.path.join(CONTENT_BASE_DIR, file_name)
        print()
        full_content = "å†…å®¹æ–‡ä»¶è¯»å–å¤±è´¥æˆ–ä¸å­˜åœ¨ã€‚"
        try:
            if os.path.exists(full_path):
                with open(full_path, 'r', encoding='utf-8') as f:
                    full_content = f.read()
            # å¦åˆ™ä¿æŒé»˜è®¤é”™è¯¯ä¿¡æ¯
        except Exception as e:
            full_content = f"è¯»å–æ–‡ä»¶ {file_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}"
        # --- å…¨æ–‡è¯»å–é€»è¾‘ç»“æŸ ---

        # ç»„è£…æœ€ç»ˆå­—å…¸
        final_results.append({
            "title": cleaned_title.strip(),
            "score": score,
            "content": full_content
        })

    return final_results


# --- ç¤ºä¾‹è°ƒç”¨ ---

if __name__ == '__main__':
    run_sztu_news_spider()
    user_query = "è¿åŠ¨ä¼š"

    print(f"--- ğŸš€ æ£€ç´¢å¼€å§‹ (æŸ¥è¯¢: '{user_query}') ---")

    results_with_content = search_jiaodian_news(
        query_text=user_query,
        top_k=3
    )

    if results_with_content:
        for i, res in enumerate(results_with_content):
            print(f"Ranking {i + 1}: (ç›¸ä¼¼åº¦: {res['score']})")
            print(f"  æ ‡é¢˜: {res['title']}")
            print(f"  å…¨æ–‡å†…å®¹ (å‰100å­—): {res['content'][:100]}...")
            print("-" * 35)
    else:
        print("æœªèƒ½æ‰¾åˆ°ç›¸ä¼¼å†…å®¹æˆ–å‘ç”Ÿé”™è¯¯ã€‚")
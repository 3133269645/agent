import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import os
import re
import openai
import numpy as np
from typing import List, Dict, Optional
from dotenv import load_dotenv  # å¯¼å…¥ dotenv åº“

load_dotenv()


# --- çˆ¬è™«è„šæœ¬ä¸»å‡½æ•° ---
def run_sztu_news_spider():
    """
    çˆ¬å–æ·±åœ³æŠ€æœ¯å¤§å­¦ (sztu.edu.cn) 'æ ¡å›­ä¸€å¡é€š' æ¿å—çš„æ–‡ç« å†…å®¹ã€‚
    å°†æ–‡ç« è¯¦æƒ…ä¿å­˜ä¸ºå•ç‹¬çš„ .txt æ–‡ä»¶ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªæ ‡é¢˜åˆ—è¡¨æ–‡ä»¶ã€‚
    è¯¥å‡½æ•°æ— ä»»ä½•å…¥å‚ï¼Œç›´æ¥è°ƒç”¨å³å¯è§¦å‘æ•´ä¸ªçˆ¬è™«æµç¨‹ã€‚
    """

    # --- 1. é…ç½®å¸¸é‡ (é›†ä¸­ç®¡ç†) ---
    BASE_URL = "https://it.sztu.edu.cn/"
    # ç›®æ ‡åˆ—è¡¨é¡µï¼šä¿¡æ¯æœåŠ¡/æ ¡å›­ä¸€å¡é€š
    TARGET_URL = urljoin(BASE_URL, "xxfw1/xyykt.htm")
    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36 Edg/141.0.0.0'
    }
    OUTPUT_DIR = "../data/text_æ ¡å›­ä¸€å¡é€š"
    TITLE_LIST_FILE = "text_title_list.txt"

    print(f"âœ… ç›®æ ‡URL: {TARGET_URL}")
    print("-" * 50)

    # --- 2. å·¥å…·å‡½æ•°å®šä¹‰ ---

    def fetch_list_page(url, headers):
        """è¯·æ±‚åˆ—è¡¨é¡µ HTML å†…å®¹ï¼Œå¤„ç†ç½‘ç»œå¼‚å¸¸ã€‚"""
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            print("ğŸ’¡ æˆåŠŸè·å–åˆ—è¡¨é¡µå†…å®¹.")
            return response.text
        except requests.RequestException as e:
            print(f"âŒ çˆ¬å–åˆ—è¡¨é¡µå¤±è´¥: {e}")
            return None

    def parse_list_page(html_content, base_url):
        """ä»åˆ—è¡¨é¡µæå–æ–‡ç« é“¾æ¥ (href, title å±æ€§) å’Œå¡ç‰‡æ˜¾ç¤ºçš„æ ‡é¢˜/æ—¥æœŸã€‚"""
        soup = BeautifulSoup(html_content, 'html.parser')

        # å®šä½æ‰€æœ‰ <li> ä¸‹çš„ <a> æ ‡ç­¾
        list_items_a = soup.select('a:has(div.text)')

        extracted_data = []
        for item_a in list_items_a:
            relative_href = item_a.get('href')
            # æå– <a> æ ‡ç­¾çš„ title å±æ€§ (ä½œä¸º fallback æ ‡é¢˜)
            title_attr = item_a.get('title')

            full_url = urljoin(base_url, relative_href) if relative_href else None

            # æå–å¡ç‰‡å†…éƒ¨çš„æ ‡é¢˜ (h6) å’Œæ—¥æœŸ (p)
            title_text = item_a.select_one('h6').text.strip() if item_a.select_one('h6') else 'N/A'
            date_summary = item_a.select_one('p').text.strip() if item_a.select_one('p') else 'N/A'

            if full_url and (title_attr or title_text != 'N/A'):
                extracted_data.append({
                    'full_url': full_url,
                    'title': title_text ,
                    'date_summary': date_summary
                })


        return extracted_data


    def save_article_file(title,url):
        """å°†æ–‡ç« å†…å®¹ä¿å­˜åˆ° [æ—¥æœŸ]_[æ¸…ç†åçš„æ ‡é¢˜].txt æ–‡ä»¶ã€‚"""
        # æ¸…ç†æ ‡é¢˜ä¸­çš„éæ³•å­—ç¬¦
        safe_title = re.sub(r'[\\/:*?"<>|]', '', title).strip()
        filename = f"{safe_title}.txt"
        filepath = os.path.join(OUTPUT_DIR, filename)

        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(f"ã€æ ‡é¢˜ã€‘: {title}\n")
                f.write(f"ã€ç½‘å€ã€‘: {url}\n\n")

            print(f"ğŸ‰ æ–‡ä»¶å·²ä¿å­˜: {filepath.replace(OUTPUT_DIR + os.path.sep, '')}")
            return True
        except Exception as e:
            print(f"âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥ ({filename}): {e}")
            return False

    def update_title_list(all_titles):
        """å°†æ‰€æœ‰æˆåŠŸå¤„ç†çš„æ–‡ç« æ ‡é¢˜ä¿å­˜åˆ°åˆ—è¡¨æ–‡ä»¶ã€‚"""
        filepath = os.path.join(OUTPUT_DIR, TITLE_LIST_FILE)

        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write("--- æ–‡ç« æ ‡é¢˜åˆ—è¡¨ (æŒ‰çˆ¬å–é¡ºåº) ---\n\n")
                for index, title in enumerate(all_titles):
                    f.write(f"{index + 1}. {title}\n")  # ä» 1 å¼€å§‹ç¼–å·
            print(f"\nâœ… æ ‡é¢˜åˆ—è¡¨æ–‡ä»¶å·²æ›´æ–°: {filepath}")
        except Exception as e:
            print(f"âŒ æ ‡é¢˜åˆ—è¡¨ä¿å­˜å¤±è´¥: {e}")

    # --- 3. ä¸»æ‰§è¡Œæµç¨‹ ---

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # 1. çˆ¬å–å¹¶è§£æåˆ—è¡¨é¡µ
    list_html = fetch_list_page(TARGET_URL, HEADERS)
    news_list = parse_list_page(list_html, BASE_URL)
    print(news_list)
    if not news_list:
        print("\nğŸš« æœªæå–åˆ°ä»»ä½•æ–‡ç« æ•°æ®ï¼Œè„šæœ¬ç»“æŸã€‚")
        return

    print(f"\nâœ¨ å‡†å¤‡å¤„ç† {len(news_list)} ç¯‡æ–‡ç« è¯¦æƒ…é¡µ âœ¨")
    print("=" * 60)

    processed_titles = []

    # 2. éå†å¹¶å¤„ç†æ‰€æœ‰æ–‡ç« 
    for item in news_list:
        title = item['title']
        url = item['full_url']
        # ä»…ä¿å­˜æˆåŠŸæå–åˆ°æ­£æ–‡çš„æ–‡ç« 
        if save_article_file(title, url):
            processed_titles.append(title)
        else:
            print(f"âš ï¸ è·³è¿‡ä¿å­˜ ({title})ï¼šæœªæå–åˆ°æœ‰æ•ˆæ­£æ–‡å†…å®¹ã€‚")

    # 3. æ›´æ–°æ ‡é¢˜åˆ—è¡¨æ–‡ä»¶
    if processed_titles:
        update_title_list(processed_titles)


# æŸ¥è¯¢å·¥å…·

EMBEDDING_MODEL = "text-embedding-3-small"
api_key = os.getenv("OPENAI_API_KEY")
TITLE_LIST_FILE = "../data/text_æ ¡å›­ä¸€å¡é€š/text_title_list.txt"
CONTENT_BASE_DIR = os.path.dirname(TITLE_LIST_FILE)

def search_school_card_text(
        query_text: str,
        top_k: int = 3
) -> List[Dict]:
    """
    é€šè¿‡è¯­ä¹‰æœç´¢ä»æ ‡é¢˜åˆ—è¡¨ä¸­æ£€ç´¢æœ€ç›¸ä¼¼çš„æ ‡é¢˜ï¼Œå¹¶è¯»å–å¯¹åº”æ–‡ä»¶çš„å…¨æ–‡å†…å®¹ã€‚
    """

    # 1. åˆå§‹åŒ–å®¢æˆ·ç«¯å’Œè¯»å–æ ‡é¢˜åˆ—è¡¨
    client = openai.OpenAI(api_key=api_key)

    try:
        with open(TITLE_LIST_FILE, 'r', encoding='utf-8') as f:
            titles_list = [line.strip() for line in f if line.strip()]
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ ‡é¢˜åˆ—è¡¨æ–‡ä»¶æœªæ‰¾åˆ°: {TITLE_LIST_FILE}")
        return []

    if not titles_list:
        print("è­¦å‘Šï¼šæ ‡é¢˜åˆ—è¡¨ä¸ºç©ºã€‚")
        return []

    # 2. ç”Ÿæˆ Embedding å¹¶è®¡ç®—ç›¸ä¼¼åº¦ (æ£€ç´¢æ­¥éª¤)
    all_texts = titles_list + [query_text]
    try:
        response = client.embeddings.create(
            input=all_texts,
            model=EMBEDDING_MODEL
        )
    except Exception as e:
        print(f"Embedding API è°ƒç”¨å¤±è´¥: {e}")
        return []

    title_embeddings = np.array([item.embedding for item in response.data[:-1]])
    query_vector = np.array(response.data[-1].embedding)
    similarity_scores = np.dot(title_embeddings, query_vector)
    ranked_indices = np.argsort(similarity_scores)[::-1]

    # 3. éå†æ£€ç´¢ç»“æœï¼Œè¯»å–å…¨æ–‡å¹¶ç»„è£…æœ€ç»ˆç»“æœ
    final_results = []

    for i in range(min(top_k, len(titles_list))):
        index = ranked_indices[i]
        title = titles_list[index]
        score = round(float(similarity_scores[index]), 4)

        cleaned_title = re.sub(r"^\d+\.\s*", "", title)

        # --- å…¨æ–‡è¯»å–é€»è¾‘ (å†…è”) ---
        file_name = f"{cleaned_title.strip()}.txt"
        full_path = os.path.join(CONTENT_BASE_DIR, file_name)
        print()
        full_content = "å†…å®¹æ–‡ä»¶è¯»å–å¤±è´¥æˆ–ä¸å­˜åœ¨ã€‚"
        try:
            if os.path.exists(full_path):
                with open(full_path, 'r', encoding='utf-8') as f:
                    full_content = f.read()
            # å¦åˆ™ä¿æŒé»˜è®¤é”™è¯¯ä¿¡æ¯
        except Exception as e:
            full_content = f"è¯»å–æ–‡ä»¶ {file_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}"
        # --- å…¨æ–‡è¯»å–é€»è¾‘ç»“æŸ ---

        # ç»„è£…æœ€ç»ˆå­—å…¸
        final_results.append({
            "title": cleaned_title.strip(),
            "score": score,
            "content": full_content
        })

    return final_results


# --- ç¤ºä¾‹è°ƒç”¨ ---

if __name__ == '__main__':
    # âš ï¸ è¯·æ ¹æ®æ‚¨çš„å®é™…é¡¹ç›®ç»“æ„ä¿®æ”¹è·¯å¾„


    # ç¡®ä¿ CONTENT_BASE_DIR èƒ½å¤Ÿæ­£ç¡®æŒ‡å‘æ–‡ç« å†…å®¹æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•
    # ä¾‹å¦‚ï¼šå¦‚æœ text_title_list.txt å’Œ .txt æ–‡ä»¶éƒ½åœ¨åŒä¸€ä¸ªç›®å½•ï¼Œåˆ™ä½¿ç”¨ä¸Šé¢çš„å®šä¹‰

    user_query = "æ ¡å›­å¡å¦‚ä½•ä½¿ç”¨å¾®ä¿¡å……å€¼"

    print(f"--- ğŸš€ æ£€ç´¢å¼€å§‹ (æŸ¥è¯¢: '{user_query}') ---")

    results_with_content = search_school_card_text(
        query_text=user_query,
        top_k=2
    )

    if results_with_content:
        for i, res in enumerate(results_with_content):
            print(f"Ranking {i + 1}: (ç›¸ä¼¼åº¦: {res['score']})")
            print(f"  æ ‡é¢˜: {res['title']}")
            print(f"  å…¨æ–‡å†…å®¹ (å‰100å­—): {res['content'][:100]}...")
            print("-" * 35)
    else:
        print("æœªèƒ½æ‰¾åˆ°ç›¸ä¼¼å†…å®¹æˆ–å‘ç”Ÿé”™è¯¯ã€‚")


